{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation Wiki on CVs and how to use them","text":"<p>Documentation in progress</p> <p>The contents of the pages are currently in development, and many aspects still in flux. </p>"},{"location":"Github_Actions/JSONLD_update/","title":"JSONLD update","text":""},{"location":"Github_Actions/JSONLD_update/#github-actions-for-jsonld-related-repositories","title":"GitHub Actions for JSONLD related repositories","text":""},{"location":"Github_Actions/JSONLD_update/#overview","title":"Overview","text":"<p>The repositories utilise GitHub Actions for continuous integration and deployment. The main workflow identified is named \"Update JSONLD.\"</p>"},{"location":"Github_Actions/JSONLD_update/#workflow-description","title":"Workflow Description","text":"<ul> <li>Name: Update JSONLD</li> <li>Purpose: This workflow is designed to update JSONLD files within the repository.</li> <li>Conditions: The workflow triggers on specific events such as commits, pull requests, or schedule (based on the standard GitHub Actions triggers).</li> </ul>"},{"location":"Github_Actions/JSONLD_update/#key-steps-and-inputs","title":"Key Steps and Inputs","text":"<ol> <li>Checkout Repository:</li> <li>Action: <code>actions/checkout@v4</code></li> <li>Inputs: <ul> <li><code>fetch-depth: 2</code></li> <li><code>repository: wolfiex/obs4MIPs-cmor-tables-ld</code></li> </ul> </li> <li> <p>Description: Clones the repository with a depth of 2 to the runner.</p> </li> <li> <p>Install Dependencies:</p> </li> <li>Action: <code>pip install</code></li> <li>Inputs: <ul> <li>Various Python dependencies such as <code>pytest</code>, <code>cmip-ld</code>, etc.</li> </ul> </li> <li> <p>Description: Installs necessary Python packages for the workflow.</p> </li> <li> <p>Run Scripts:</p> </li> <li>Action: Custom scripts (e.g., <code>write_ancillary_C3S-GTO-ECV.py</code>, <code>obs4MIPsLib.py</code>)</li> <li>Inputs: <ul> <li>Specific scripts and their associated parameters.</li> </ul> </li> <li>Description: Executes scripts to process data, update files, or perform other tasks.</li> </ol>"},{"location":"Github_Actions/JSONLD_update/#mermaid-diagrams","title":"Mermaid Diagrams","text":""},{"location":"Github_Actions/JSONLD_update/#workflow-diagram","title":"Workflow Diagram","text":"<pre><code>graph TD\n    A[Start] --&gt; B[actions/checkout@v4]\n    B --&gt; C[pip install dependencies]\n    C --&gt; D[Run custom scripts]\n    D --&gt; E[End]</code></pre>"},{"location":"Github_Actions/JSONLD_update/#detailed-step-diagram","title":"Detailed Step Diagram","text":"<pre><code>graph TD\n    A[Start] --&gt; B[actions/checkout@v4]\n    B --&gt; C1[fetch-depth: 2]\n    B --&gt; C2[repository]\n    C1 --&gt; D[pip install dependencies]\n    C2 --&gt; D\n    D --&gt; E1[Install pytest]\n    D --&gt; E2[Install cmip-ld]\n    F[Run custom scripts]\n    F --&gt; G1[script1.py]\n    F --&gt; G2[script2.py]\n    E1 --&gt; F\n    E2 --&gt; F\n    G1 --&gt; H[Trigger Publish Workflow]\n    G2 --&gt; H\n    H --&gt; I[End]</code></pre>"},{"location":"How_to_use_JSONLD/filestructure/","title":"Filestructure","text":""},{"location":"How_to_use_JSONLD/filestructure/#structure-each-directory","title":"Structure (Each directory)","text":"<p>### ld usage files    - graph.jsonld   - version.jsonld   - frame.jsonld   - schema.jsonld</p> <p>### data files   - id.json   - ...   - idn.json</p> <p>## Graph   Combined jsonld file including relevant context</p> <p>## Version    Versioning history of all the individual json files. </p> <p>## Frame    Necessary for recognition scripts.    Provides Vocab used in Graph    Example Usage </p> <p>## Schema   JSON Schema for checking conformance. </p>"},{"location":"What_is_JSONLD%3F/Use_and_Changes/","title":"Use and Changes","text":""},{"location":"What_is_JSONLD%3F/Use_and_Changes/#json-linked-data","title":"JSON Linked-Data","text":"<p>JSONLD is a lightweight link data format closely conforming to the JSON standards. It allows us to provide relational data, in a flat format accessible to everyone within minimal intervention. </p>"},{"location":"What_is_JSONLD%3F/Use_and_Changes/#why-is-this-useful","title":"Why is this useful?","text":"<p>The JavaScript Object Notation has been accepted as a human and machine readable and intuitive data standard for both scripting and online programs. It tends to describe items in a Key-Value pair structure and present it in plain-text.  This is the standard that was adopted for CMIP6 control variables and has been used since. </p> <p>In converting current and future data into JSONLD we are able to provide pointers to reusable bits of information, thus drastically reducing duplication, and in turn human-induced error in the WCRP (and beyond) community. </p>"},{"location":"What_is_JSONLD%3F/Use_and_Changes/#existing-workflows","title":"Existing Workflows","text":"<p>In introducing any new technology, there is often a period in which existing software will need to be updated. By opting for JSONLD the interference between this should be minimal, since any produced files will still be readable by existing scripts. </p>"},{"location":"What_is_JSONLD%3F/Use_and_Changes/#bespoke-output-and-breaking-changes","title":"Bespoke output and Breaking Changes","text":"<p>The main merit of JSONLD comes from its framing capabilities. It allows us to take a flat JSON file and build (populate) this with all linked components without having to store them in the same file, or even location.  JSONLD parsers are available for all majour programming languages, and capable of extracting linked files (using their ids (URIs)) meaning that we will always have an up-to-date frame when we request it. </p> <p>This also means that should a non-standard or altered format be required for your work (e.g. just the names of MIPs or Institutions in an Activity, this will be possible through either custom framing or requesting an action be added to the relevant repository to generate this. Github actions are designed to run each time the data in the repository is updated. </p>"},{"location":"What_is_JSONLD%3F/Use_and_Changes/#how-does-it-change-the-existing-structure","title":"How does it change the existing structure?","text":"<p>JsonLD is still a valid JSON format allowing all previous tools and workflows to function. .  In addition to this we apply a intuitive unique id to each item, and a context file. </p>"},{"location":"What_is_JSONLD%3F/Use_and_Changes/#example-file-change-to-ld","title":"Example file change to LD","text":""},{"location":"What_is_JSONLD%3F/Use_and_Changes/#context-files","title":"Context files","text":""},{"location":"What_is_JSONLD%3F/Use_and_Changes/#other-changes-to-the-cvs","title":"Other changes to the CVs","text":""},{"location":"What_is_JSONLD%3F/Use_and_Changes/#file-hierarchy","title":"File hierarchy","text":"<p>Instead of having one large file containing all possible sources/experiments/institutions etc., we will break these out into individual json files. </p> <p>This serves to improve the workflow and usability of the directories in several ways: </p> <ol> <li>A better understanding of changes over time. This way we are able to create a difference log for an individual item, without having to track the entire collection or category.</li> <li>Error isolation A mistake in a single file will not affect any of the others, and will allow us to easier identify it. </li> <li>Versioning The changes in each file are tracked, and we know exactly when they have occured. </li> <li>Line identification It will be easier to hilight a file or specific line, without the worry that an issue 2 weeks old will now refer to something else. </li> <li>ATTRIBUTION by seeing who is contributing changes to certain files will allow us to credit prominent members of the community, and generate a list of specialists for specific topics should there be any queries at a later point. </li> </ol>"},{"location":"delete/99_Acknowlegements/","title":"Acknowledgement","text":"<p>The work within most repositories can be attributed to a large number of contributers. A collection of those involved can be found here. </p>"},{"location":"delete/99_Acknowlegements/#mip-cmor-tables","title":"MIP CMOR Tables","text":"<p>The repository content has been developed by climate and computer scientists representing the Coupled Model Intercomparison Project phase 6 (CMIP6) and earlier phases, including those from climate modeling groups and model intercomparison projects (MIPs) worldwide. A special mention to Dr. Martin Juckes from the UK Centre for Environmental Data Analysis (CEDA) for leading efforts in the CMIP6 Data Request. The structure of repository content and tools required to maintain it was developed by climate and computer scientists from the Program for Climate Model Diagnosis and Intercomparison (PCMDI) at Lawrence Livermore National Laboratory (LLNL) and the UK MetOffice, with assistance from colleagues at the Coupled Model Intercomparison Project International Project Office (CMIP-IPO), the Deutsches Klimarechenzentrum (DKRZ) in Germany and the members of the Infrastructure for the European Network for Earth System Modelling (IS-ENES) consortium.</p> <p>This work is sponsored by the Regional and Global Model Analysis (RGMA) program of the Earth and Environmental Systems Sciences Division (EESSD) in the Office of Biological and Environmental Research (BER) within the Department of Energy's (DOE) Office of Science (OS). The work at PCMDI is performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.</p> <p> </p>"},{"location":"delete/99_Acknowlegements/#cmip6plus-cvs","title":"CMIP6Plus CVS","text":"<p>The repository content has been collected from many contributors representing the Coupled Model Intercomparison Project phase 6+ (CMIP6Plus), including those from climate modeling groups and model intercomparison projects (MIPs) worldwide. The structure of content and tools required to maintain it was developed by climate and computer scientists from the Coupled Model Intercomparison Project International Project Office (CMIP-IPO), the Program for Climate Model Diagnosis and Intercomparison (PCMDI) at Lawrence Livermore National Laboratory (LLNL), and the UK MetOffice, with assistance from colleagues at the UK Centre for Environmental Data Analysis (CEDA), the Deutsches Klimarechenzentrum (DKRZ) in Germany and the members of the Infrastructure for the European Network for Earth System Modelling (IS-ENES) consortium.</p> <p>This work is sponsored by the Regional and Global Model Analysis (RGMA) program of the Earth and Environmental Systems Sciences Division (EESSD) in the Office of Biological and Environmental Research (BER) within the Department of Energy's (DOE) Office of Science (OS). The work at PCMDI is performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344.</p> <p> </p>"},{"location":"delete/99_Acknowlegements/#wiki-forms-automations-and-infrastructure-work","title":"Wiki, Forms, Automations, and Infrastructure work.","text":"<p>CMIP IPO (Daniel Ellis), LLNL (Paul Durack), MOHC (Matthew Mizielinski)</p>"},{"location":"delete/CMIP6Plus/","title":"What is CMIP6Plus","text":"Description Repository Status <p>This page outlines building guidance to register and contribute to the CMIP6Plus phase. We will update this page with new information as it becomes available.</p>"},{"location":"delete/CMIP6Plus/#current-guidance-for-contributors","title":"Current guidance for contributors:","text":"<ul> <li>Add a new model/source_id</li> </ul>"},{"location":"delete/CMIP6Plus/#additional-resources","title":"Additional resources:","text":"<ul> <li>MIP tables</li> <li>Registered Model Intercomparison Project (MIPs)</li> </ul>"},{"location":"delete/CMIP6Plus/Automations/creating_a_dispatch_automation/","title":"Creating a dispatch action","text":"<p>A dispatch action is one where a call to the github API triggers an action, and consequently a script.  Our usage case for CMIP6Plus is the submission of new data from a web form or API to update a file. </p>"},{"location":"delete/CMIP6Plus/Automations/creating_a_dispatch_automation/#how-to-submit","title":"How to  submit","text":"<p>Here we have an event payload - this contains any data we want to send. Here the entries we will add to a table are contained in the <code>jsondata</code> field. </p>"},{"location":"delete/CMIP6Plus/Automations/creating_a_dispatch_automation/#javascript","title":"Javascript","text":"<pre><code>async function triggerWorkflow() {\n  const repoOwner = owner;\n  const repoName = repo;\n  const personalAccessToken = githubToken;\n\n  const apiUrl = `https://api.github.com/repos/${repoOwner}/${repoName}/dispatches`;\n  console.log(apiUrl)\n  const eventPayload = {\n    event_type: 'source_id',\n    client_payload: {\n      record: jsondata ,\n      author:  'CMIP-IPO Automation',\n      name: myrecord\n\n      ...  insert other fields here \n\n    },\n  };\n\n  const options = {\n    method: 'POST',\n    headers: {\n      'Accept': 'application/vnd.github.everest-preview+json',\n      'Authorization': `token ${personalAccessToken}`,\n      'Content-Type': 'application/json',\n    },\n    body: JSON.stringify(eventPayload),\n  };\n</code></pre>"},{"location":"delete/CMIP6Plus/Automations/creating_a_dispatch_automation/#python","title":"Python","text":"<pre><code>import requests\nimport json\n\nasync def trigger_workflow():\n    repo_owner = owner\n    repo_name = repo\n    personal_access_token = github_token\n\n    api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/dispatches\"\n    print(api_url)\n\n    event_payload = {\n        \"event_type\": \"source_id\",\n        \"client_payload\": {\n            \"record\": jsondata,\n            \"author\":  'CMIP-IPO Automation',\n            \"name\": my_record\n        }\n    }\n\n    headers = {\n        \"Accept\": \"application/vnd.github.everest-preview+json\",\n        \"Authorization\": f\"token {personal_access_token}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    response = requests.post(api_url, headers=headers, data=json.dumps(event_payload))\n\n    if response.status_code == 204:\n        print(\"Workflow triggered successfully.\")\n    else:\n        print(f\"Failed to trigger workflow. Status code: {response.status_code}\")\n        print(response.text)\n\n# Call the function\nawait trigger_workflow()\n</code></pre>"},{"location":"delete/CMIP6Plus/Automations/creating_a_dispatch_automation/#action-script","title":"Action script","text":"<p>The action script is contained in the YML file under your <code>.github/workflows</code> folder.  To trigger a new dispatch and read the event payload we use the following script. </p> <pre><code>name: Source_ID_dispatch\n\non:\n  repository_dispatch:\n    types:\n      - source_id\n\njobs:\n  process-payload:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v2\n\n      - name: Print Payload\n        run: |\n          echo \"Received Payload:\"\n          echo \"Key: ${{ github.event.client_payload.key }}\"\n          echo \"json: ${{ github.event.client_payload.record }}\"\n          echo \"author: ${{ github.event.client_payload.author }}\"\n          echo \"source: ${{ github.event.client_payload.name }}\"\n</code></pre> <p>Accessing the values can be done using <code>github.event.client_payload</code>.</p> <p>We can now feed these into a python script of our choosing:  <pre><code>       - name: Run Python Check\n         id: 'run-python-script'\n         run: |\n\n           python .github.lib/source_id.py -k ${{ github.event.client_payload.key }} -r ${{ github.event.client_payload.record }}\n\n         env:\n           PYTHON_SCRIPT_OUTPUT: ${{ steps.run-python-script.outputs.stdout }}\n           PYTHON_SCRIPT_ERROR: ${{ steps.run-python-script.outputs.stderr }}\n         continue-on-error: false\n</code></pre></p>"},{"location":"delete/CMIP6Plus/Automations/gencv_action/","title":"[Action] To run create_CV","text":"<p>The flow of the github action which run when a new commits are pushed to a branch. </p> <pre><code>graph TD\n  style AA fill:#003366,stroke:#ffffff,stroke-width:2px;\n  style BB fill:#005cbf,stroke:#ffffff,stroke-width:2px;\n  style CC fill:#0078d4,stroke:#ffffff,stroke-width:2px;\n  style DD fill:#009be1,stroke:#ffffff,stroke-width:2px;\n  style EE fill:#00a8e8,stroke:#ffffff,stroke-width:2px;\n  style FF fill:#00adef,stroke:#ffffff,stroke-width:2px;\n  style GG fill:#33b8ff,stroke:#ffffff,stroke-width:2px;\n  style HH fill:#66c1ff,stroke:#ffffff,stroke-width:2px;\n  style II fill:#99ccff,stroke:#ffffff,stroke-width:2px;\n\n  A[Checkout Repository] --&gt;|1. Checkout| B[Check if Run is Necessary]\n  B --&gt;|2. Determine Necessity| C[Set Up Git]\n  C --&gt;|3. Configure Git Settings| D[Set GIT Repo Environment Variables]\n  D --&gt;|4. Set Environment Variables| E[Display GIT Environment Variables]\n  E --&gt;|5. Display Variables| F[Print Latest Commit SHA]\n  F --&gt;|6. Print SHA and Path| G[Run Python Check]\n  G --&gt;|7. Execute Python Script| H[Write New CV]\n  H --&gt;|8. Write and Commit if Necessary| I[End]\n\n  style A fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style B fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style C fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style D fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style E fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style F fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style G fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style H fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n  style I fill:#4CAF50,stroke:#ffffff,stroke-width:2px;\n\n  A --&gt;|Begin Workflow| B\n  B --&gt;|Determine Necessity based on Git History| C\n  C --&gt;|Configure Git Settings| D\n  D --&gt;|Set Environment Variables| E\n  E --&gt;|Display Variables| F\n  F --&gt;|Print Commit SHA and Path| G\n  G --&gt;|Execute Python Script| H\n  H --&gt;|Write and Commit if Necessary| I\n\n  subgraph AA\n    A\n  end\n\n  subgraph BB\n    B\n  end\n\n  subgraph CC\n    C\n  end\n\n  subgraph DD\n    D\n  end\n\n  subgraph EE\n    E\n  end\n\n  subgraph FF\n    F\n  end\n\n  subgraph GG\n    G\n  end\n\n  subgraph HH\n    H\n  end\n\n  subgraph II\n    I\n  end</code></pre>"},{"location":"delete/CMIP6Plus/Automations/generate_cv/","title":"[Py] Generate CV","text":"<p>The python script used to generate a cv. </p> <p>This is run by the action script [link here]. </p>"},{"location":"delete/CMIP6Plus/Automations/generate_cv/#arguments","title":"Arguments","text":"<pre><code>Create CV \nusage: create_cv.py [-h] [-c COMMIT] [-d DATE] [-t TAG] [-b BRANCH] [-a API]\n\nGithub action script to create CVs\n\noptions:\n  -h, --help            show this help message and exit\n  -c COMMIT, --commit COMMIT\n                        Commit SHA\n  -d DATE, --date DATE  date_commit\n  -t TAG, --tag TAG     tag\n  -b BRANCH, --branch BRANCH\n                        branch\n  -a API, --api API     api_token\n</code></pre>"},{"location":"delete/CMIP6Plus/Automations/generate_cv/#program-flow","title":"Program Flow","text":"<pre><code>\ngraph TB\n\nsubgraph \"Initialize Parameters\"\n    A[Set relative path]\n    B[Set CV prefix]\n    C[Define MIP tables prefix]\n    D[Define table prefix pattern]\nend\n\nsubgraph \"Argument Parsing\"\n    E[Parse commit, date, tag, branch, and API token]\nend\n\nsubgraph \"Define Functions\"\n    F[Read contents from GitHub]\n    G[Read JSON from GitHub]\n    H[Listify function]\n    I[Notnull function]\nend\n\nsubgraph \"Tunable Parameters\"\n    J[Define additional parameters]\nend\n\nsubgraph \"Other Parameters\"\n    K[Define additional parameters]\nend\n\nsubgraph \"Read from MIP Tables\"\n    L[Read source_type, frequency, realm, etc. from MIP tables]\nend\n\nsubgraph \"Main Section\"\n    M[Loop through structure elements]\n    N[Read table_id from GitHub]\n    O[Loop through experiments and update source_type]\n    P[Handle experiment_id entries]\n    Q[Handle activity_id entries]\n    R[Handle source_id entries]\nend\n\nsubgraph \"Metadata and Checksum\"\n    S[Get latest commit from MIP tables]\n    T[Update version metadata]\n    U[Calculate checksum]\nend\n\nsubgraph \"Checksum\"\n    V[Extract branch from branch argument]\n    W[Remove old branched CVs if branch is main]\n    X[Calculate checksum and compare with the old CV]\n    Y[Write updated CV to file]\nend\n\nA --&gt; E\nB --&gt; E\nC --&gt; E\nD --&gt; E\nE --&gt; J\nJ --&gt; L\nE --&gt; F\nE --&gt; G\nJ --&gt; F\nJ --&gt; G\nJ --&gt; H\nJ --&gt; I\nL --&gt; M\nN --&gt; M\nM --&gt; O\nO --&gt; P\nP --&gt; Q\nQ --&gt; R\nR --&gt; T\nT --&gt; U\nU --&gt; V\nV --&gt; W\nW --&gt; X\nX --&gt; Y\n</code></pre>"},{"location":"delete/CMIP6Plus/Rules/activity_id/","title":"Activity ID","text":"Instructions <p>The rule guides describe what is required to submit a new item, and how to correctly fill out the forms and avoid submission errors. </p>"},{"location":"delete/CMIP6Plus/Rules/activity_id/#name","title":"Name","text":"<p>This is where you specify the activity name. This must be unique and must not change. Often acronyms (e.g.<code>CMIP</code> and <code>LESFMIP</code>) are used to describe the activity. </p> <p>Rules :  Unicode, <code>[A-z]</code>, case-sensitive.  </p>"},{"location":"delete/CMIP6Plus/Rules/activity_id/#activity-url","title":"Activity URL","text":"<p>To aid users to find out more about the activity, in CMIP6Plus we are requiring a URL describing it. This can be a fixed page on a website (ideal) or a link to the definition paper. </p> <p>Rules :  Existing URL string. Ideally this will work with both <code>http</code> and <code>https</code> </p>"},{"location":"delete/CMIP6Plus/Rules/activity_id/#long-name","title":"Long Name","text":"<p>Finally we require a long (desciptive) name of the activity that may be used to better identify the activity. </p> <p>Rules :  Unicode, free-form, text. </p>"},{"location":"delete/CMIP6Plus/Rules/institution/","title":"Institutions","text":"Instructions <p>The rule guides describe what is required to submit a new item, and how to correctly fill out the forms and avoid submission errors. </p> <p>New institutions are added to the cmor-mip-tables. </p> <p>To check if an institutions has already been added, try adding its ROR code and acronym in the [new institution] (add link here) submission form. </p>"},{"location":"delete/CMIP6Plus/Rules/institution/#acronym","title":"Acronym","text":"<p>This is what we shall use to identify an institution within relevant scripts. This must be unique and should not change. </p> Once data has been submitted using this handle, it cannot be changed, even if your institute rebrands. <p> <p>Rules :  Unicode, <code>[A-z]</code>, case-sensitive. Max 25 (tbc) characters. </p>"},{"location":"delete/CMIP6Plus/Rules/institution/#full-name","title":"Full Name","text":"<p>This is the full name of the institution. This should match &gt;90% with one of the approved names in ROR. </p> <p>Rules :  Unicode, <code>[A-z]</code>, case-sensitive. Max 25 (tbc) characters. </p>"},{"location":"delete/CMIP6Plus/Rules/institution/#ror","title":"ROR","text":"<p> Research Organisation Repository is a global, community-led registry of open persistent identifiers for research organizations. This gives us an ability to assign a unique identifier to each institution, which persists even after they have rebranded, merged, split, shut down, and re-emerged. </p> <p>Rules :  An alphanumeric string, 9 characters, defined by ROR.org </p>"},{"location":"delete/Data-Analysis-Tools/","title":"FEoCMIP-Data-Analysis-Tools","text":""},{"location":"delete/Data-Analysis-Tools/#this-repository-contains-data-handling-tools-for-cmip-models-contributed-by-members-of-fresh-eyes-on-cmip","title":"This repository contains Data handling tools for CMIP models, contributed by members of Fresh Eyes on CMIP","text":""},{"location":"delete/Data-Analysis-Tools/#this-repository-is-organized-into-the-following-categories","title":"This repository is organized into the following categories:","text":"<ul> <li>Preprocessing</li> <li>Analysis</li> <li>Visualization</li> <li>(ADD MORE HERE)</li> </ul>"},{"location":"delete/Data-Analysis-Tools/#each-python-script-added-should-include-a-env-file-as-well","title":"Each python script added should include a .env file as well","text":""},{"location":"delete/Data-Analysis-Tools/#the-general-layout-of-each-file-is-as-follows","title":"The general layout of each file is as follows:","text":"<ol> <li>Name and contact email of main contributor(s)</li> <li>Purpose of script/function</li> <li>Any other specific information users will need</li> <li>Executable code</li> </ol>"},{"location":"delete/Data-Analysis-Tools/#contributor-information-and-affiliations","title":"Contributor Information and Affiliations","text":"<ul> <li>Keighan Gemmell, University of British Columbia, Canada \ud83c\udde8\ud83c\udde6</li> <li>J\u00falia Crespin Esteve, Universitat de Barcelona, Spain</li> <li>Anja Katzenberger, Potsdam Institute of Climate Impact Research, Germany </li> <li>ADD YOUR NAME AND AFFILIATION HERE </li> </ul>"},{"location":"delete/Data-Analysis-Tools/Preprocessing/preprocess_parallel/","title":"preprocess parallel","text":"<p>This file contains a wrapped preprocessing function that detrends, removes seasonal cycle, and executes a rolling average</p>"},{"location":"delete/Data-Analysis-Tools/Preprocessing/preprocess_parallel/#note-on-running","title":"Note on running:","text":"<ul> <li>There is also a normalization function </li> <li>To run this script you will need ensure xarray is installed</li> <li></li> </ul> <p>Note</p> <p>Remember to change file paths to where your data is stored </p> <pre><code>    #Created by Keighan Gemmell (keighan@chem.ubc.ca)\n    #This file contains a wrapped preprocessing function that detrends, removes seasonal cycle, and executes a rolling average\n    #There is also a normalization function \n    #To run this script you will need ensure xarray is installed\n    #Change file paths to where your data is stored \n\n    import xarray as xr\n    import numpy as np\n    from multiprocessing import Pool\n    import s3fs\n    import geocat.comp\n    import time\n    import pandas as pd\n    import json\n    import os\n    import sys\n    import pyximport\n    pyximport.install(setup_args={\"include_dirs\":np.get_include()},\n                    reload_support=True)\n    sys.path.insert(0,'') #fill in path here\n\n    ### Set of functions for executing preprocessing of CMIP6 data\n    ### This code used an old AWS S3 storage system to access CMIP6 data, you will need to add the lines to pull data from wherever you store it\n\n    df = pd.read_csv(\"\") #name of file here\n\n    proc = 40\n\n    def getData(query:dict):\n        '''\n        Load CMIP6 data into xarray dataframe\n        query (dict or str) - dict or str with data information\n                            - if dict format as {'param':'value','param2':['val1','val2']}\n        '''\n        # Create query string for pandas.DataFrame.query\n        if type(query) is dict:\n            inputStr = \" &amp; \".join([\"{0}=='{1}'\".format(param, query[param]) for param in query])\n        elif type(query) is str: # if its already a string, pass through\n            inputStr=query\n\n        # Searches cmip6 data csv for datasets that match given parameters\n        df_subset = df.query(inputStr)\n        if df_subset.empty:\n            print('data not available for '+inputStr)\n        else:\n            # load data\n            for v in df_subset.zstore.values:\n                zstore = v\n                mapper = fs.get_mapper(zstore)\n                ### !!!! Note decode times is false so we can use integer time values !!!!\n                ### open_zarr, so datasets are not loaded yet\n                return_ds = xr.open_zarr(mapper, consolidated=True,decode_times=False)\n        return(return_ds)\n\n    def removeSC(x:np.ndarray):\n        '''\n        Removes seasonal cycle from monthly data\n        x (np.ndarray) - 3D (time,lat,lon) numpy array\n        '''\n        nt,nx,ny = x.shape # get array dimensions\n        nyears = nt//12\n        # get month means\n        monmean = np.mean(x.reshape(nyears,12,nx,ny),axis=0)\n        for m in range(12): #for each month\n            x[m::12] = x[m::12] - monmean[m]\n        return x\n\n    def detrend(x:np.ndarray,time:np.ndarray):\n        '''\n        remove degree three polynomial fit\n        x (np.ndarray) : 3D (time, lat, lon) numpy array\n        time (np.ndarray) : 1D (time) array \n        '''\n        nt,nx,ny = x.shape\n        xtemp = x.reshape(nt,nx*ny)\n        p = np.polyfit(time, xtemp, deg=3)\n        print(p.shape)\n        fit = p[0]*(time[:,np.newaxis] **3)+ p[1]*(time[:,np.newaxis]**2) + p[2]*(time[:,np.newaxis]) + p[3]\n        return x - fit.reshape(nt,nx,ny)\n\n    def preprocess(data:np.ndarray, tdata:np.ndarray, window:int = 31,proc:int = 40):\n        '''\n        Executes the three steps of the preprocessing (deseasonalize, detrend, normalize)\n        data (np.ndarray) : data to process\n        t (np.ndarray) : time values\n        window (integer) : years for the rolling average\n        proc (int) : number of processes for multiprocessing\n        '''\n\n        ### remove seasonal cycle\n        t1 = time.time()\n        print('Deseasonalize')\n        deseas = removeSC(data)\n        t2 = time.time()\n        print('Time : ', t2-t1)\n\n        ### remove cubic fit\n        print('Detrend')\n        print(tdata.shape)\n        detre = detrend(deseas,tdata)\n        t3 = time.time()\n        print('Time : ',t3-t2)\n\n        x = detre\n\n        ## function for normalizing - written for passing to multiprocessing\n        global normalize\n        def normalize(t):\n            '''\n            Calculate anomaly and normalize (repeat boundary values) for monthly data\n            averages across years (12 time step skips)\n            x (np.ndarray) - integer values\n            t (integer) - time\n            window (integer) - years in the averaging window must be odd for now\n            '''\n            assert window%2 == 1\n            tmax = x.shape[0]\n            halfwindow = window//2\n            yr = t//12\n            mon = t%12\n            selx = np.zeros_like(x[:window])\n            # get rolling window (backfills/forward fills with first/last value)\n            if t-halfwindow*12 &lt; 0:\n                selx[:halfwindow - yr] = x[mon]\n                selx[halfwindow-yr:] = x[mon:t+halfwindow*12+1:12]\n            elif t+halfwindow*12+1 &gt; tmax:\n                selx[halfwindow + (tmax//12 - yr):] = x[tmax- 12 + mon]\n                selx[:halfwindow + (tmax//12 - yr)] = x[t - halfwindow*12::12]\n            else:\n                selx = x[t-halfwindow*12:t+halfwindow*12+1:12]\n            # calculate normalized\n            normed = (x[t] - np.mean(selx,axis=0))/np.std(selx,axis=0)\n            return normed\n        ### run normalization\n        print('Normalize')\n        ntime = x.shape[0]\n\n        # parallelize normalizing across time (trivially parallel)\n        with Pool(processes=proc) as pool: \n            outdata = pool.map(normalize, range(ntime))\n        t4 = time.time()\n        print('Time : ',t4-t3)\n        return np.array(outdata,dtype=np.float32)\n\n    def run_preprocess(experiment:str, modelName:str, member:str, variables_in:list,\n                variables_out:list, attribute_path:str = 'variable_defs.json',\n                out_path:str = None,lf_query:str = None,\n                append:bool=False,frequency:str='Amon'):\n        '''\n        Wrapper function that executes the preprocessing pipeline\n        experiment (str) : experiment name (experiment_id)\n        modelName (str) : model name (source_id)\n        member (str) : ensemble member code (member_id) r*i*p*f*\n        variables_in (list) : list of variables to grab from CMIP6 archive\n        variables_out (list) : list of variables to calculate and output\n        attribute_path (str) : path to a json file with variable descriptions.\n            Note if you request a variable in variables_out that is not a CMOR variable, it must\n            be defined in the attribute .json\n        out_path (str) : file name for output\n        lf_query (str) : query string to pass to pandas.DataFram.query  recommended to pass this,\n            otherwise the script is likely to fail due to not finding sftlf variable\n            set to False to skip adding the landfraction, set to None to attempt to find land fraction\n            for the selected experiment and ensemble member\n        append (str) : append variables_out to an existing netcdf file\n        frequency (str) : CMIP table to get data from (e.g. Amon, Omon, Aday - table_id). \n        '''\n        # Default out_path\n        if out_path is None: \n            out_path = ''.format(modelName, member, experiment)\n\n        print('Preprocessing for {0} {1} {2}'.format(modelName,experiment,member))\n\n        # if we are appending to an existing netcdf\n        # load the dataset and determine the existing variables\n        if append:\n            if os.path.isfile(out_path):\n                existing = xr.open_dataset(out_path)\n                existing_variables = list(existing.variables)\n                del existing\n            else:\n                print('append = True, but no existing file. Setting to append = False')\n                append = False\n\n        ## Load in data (could make faster with open_mfdataset?)\n        dict_query = {'source_id':modelName, 'table_id':frequency, 'experiment_id':experiment, 'member_id':member}\n        data = {}\n        for var in variables_in:\n            dict_query['variable_id'] = var\n            data[var] = getData(dict_query)\n\n        # Load land fraction data\n        if lf_query is None:\n            # if query isn't provided, attempt to load from current experiment\n            lf_query  = \"source_id=='{0}' &amp; table_id=='fx' &amp; experiment_id=='{1}' &amp;  member_id=='{2}' &amp; variable_id=='sftlf'\".format(modelName,experiment,member)\n        elif lf_query: # set lf_query=False to skip\n            data['sftlf'] = getData(lf_query) ## land area fraction as %\n\n        # Attributes for derived variables\n        d_attr = json.load(open(attribute_path))\n\n        processed = {} # preprocessed data\n        orig = {} # raw data\n        for var in variables_out:\n            if append and var in existing_variables:\n                # skip existing variables if in append mode\n                print('{0} already exists, skipping'.format(var))\n                variables_out.remove(var)\n                continue\n            print('-----------------')\n            print(var)\n            print('-----------------')\n            if var in variables_in: \n                # If taking variable directly from CMIP\n                signs = [1]\n                variables = [var]\n                attributes = {'long_name': data[var][var].long_name, 'units':data[var][var].units}\n            elif var in d_attr: \n                # If calculating a variable from CMIP variables\n                signs = d_attr[var]['signs']\n                attributes = d_attr[var]\n                variables = d_attr[var]['variables']\n            else: # Something's wrong (missing variable or definition)\n                print('missing variable definition in {0}'.format(attribute_path))\n                continue\n            if 'integral' in attributes:\n                # if we are integrating a 3D variable\n                processed[var], orig[var] = calc_var_integral(data,variables[0],var,attributes = attributes)\n            else:\n                processed[var], orig[var] = calc_var(data, variables, signs, var,attributes = attributes)\n\n        # Cast the fixed land fraction data into a time series\n        if lf_query: # set lf_query=False to skip\n            # tile in time\n            sftlfOut, b2 = xr.broadcast(data['sftlf'].sftlf, data[variables_in[0]][variables_in[0]], exclude=('lat','lon'))\n            sftlfOut=sftlfOut.where(sftlfOut==0,1) ### data has values as 0 for ocean or 100 for land sa making 100 as 1\n            lsMask = xr.DataArray(name='lsMask', data=sftlfOut.load(), attrs={'long_name':'land_sea_mask', 'Description':'land fraction of each grid cell 0 for ocean and 1 for land'}, \n                                coords={'time': data[variables_in[0]].time,'lat': data['sftlf'].lat,'lon': data['sftlf'].lon})\n\n        # change time to original values (time values get altered in calc_var)\n        for var in variables_out:\n            processed[var]['time'] = data[variables_in[0]].time\n            orig[var]['time'] = data[variables_in[0]].time\n\n        # Save one DataArray as dataset\n        var = variables_out[0]\n        output_ds = processed[var].to_dataset(name = var+'_pre')\n        output_ds[var] = orig[var]\n        for var in variables_out[1:]:\n            # Add next DataArray to existing dataset (ds)\n            output_ds[var+'_pre'] = processed[var]\n            output_ds[var] = orig[var]\n        # add the landfraction query\n        if lf_query:\n            output_ds['lsMask'] = lsMask\n        print('Write to file')\n        if append:\n            output_ds.to_netcdf(path=out_path,mode='a',format='NETCDF4')\n        else:\n            output_ds.to_netcdf(path=out_path,mode='w',format='NETCDF4')\n</code></pre>"},{"location":"delete/Data-Analysis-Tools/Visualisation/change_in_windvector/","title":"Change in Wind speed and Direction","text":"<p>Created by anjakatzenberger</p> <p>This code provides changes in wind speed and wind direction for a subset of CMIP6 models</p> <ol> <li>The first figure (a panel of 2x3 wind fields) is written for 6 CMIP6 models</li> <li>The second figure gives a multi model mean of the models given</li> <li>The input data is preprocessed using CDOs (selected time period, selected altitude of winds, as well as mean over time for specific months)</li> </ol>"},{"location":"delete/Data-Analysis-Tools/Visualisation/change_in_windvector/#adapt-the-preprocessing-as-adequate-for-your-research-question","title":"Adapt the preprocessing as adequate for your research question","text":""},{"location":"delete/Data-Analysis-Tools/Visualisation/change_in_windvector/#example-template-of-the-program-structure","title":"Example template of the program structure.","text":"<pre><code>graph TD\n  subgraph \"Initialize Directories\"\n    A[udir_future]\n    B[vdir_future]\n    C[udir]\n    D[vdir]\n    E[dir_save]\n  end\n\n  subgraph \"List Files\"\n    F[List files in udir_future]\n    G[List files in vdir_future]\n    H[List files in udir]\n    I[List files in vdir]\n  end\n\n  subgraph \"Create Wind Fields\"\n    J[Loop through models]\n    K[Load u and v data]\n    L[Calculate wind speed]\n    M[Plot wind fields]\n    N[Save figure]\n  end\n\n  subgraph \"Multi-Model Mean\"\n    O[Create arrays for wind speed, ua_diff, va_diff]\n    P[Loop through models]\n    Q[Load u and v data]\n    R[Calculate ua_diff and va_diff]\n    S[Calculate mean wind speed]\n    T[Plot mean wind speed]\n    U[Save figure]\n  end\n\n  A --&gt; J\n  B --&gt; J\n  C --&gt; J\n  D --&gt; J\n  E --&gt; M\n  F --&gt; J\n  G --&gt; J\n  H --&gt; J\n  I --&gt; J\n  J --&gt; K\n  K --&gt; L\n  L --&gt; M\n  M --&gt; N\n  N --&gt; |Repeat for each model| J\n  O --&gt; P\n  P --&gt; Q\n  Q --&gt; R\n  R --&gt; O\n  R --&gt; S\n  S --&gt; T\n  T --&gt; U\n</code></pre>"},{"location":"delete/MIP_Tables/","title":"mip-cmor-tables","text":"<p>JSON tables to create Model Intercomparison Project (MIP) datasets</p> <p>A quick intro on how to make use of these tables is available on the Wiki.</p>"},{"location":"delete/MIP_Tables/#contributors","title":"Contributors","text":"<p>Thanks to our contributors!</p>"},{"location":"delete/MIP_Tables/advanced_search/","title":"Advanced Searching","text":"<p>To perfom the search we use the Lunr search library.  Lunr is a lightweight, client-side search library for web applications using an inverted index data structure, enabling efficient full-text searches directly within the browser.</p>"},{"location":"delete/MIP_Tables/advanced_search/#wildcard-searching","title":"Wildcard searching","text":"<p>We can use a wildcard anywhere in a query to capture everything  <code>path/to/data/MIP_*.json</code></p>"},{"location":"delete/MIP_Tables/advanced_search/#sepecifying-columns-fields","title":"Sepecifying Columns (fields)","text":"<p>We can specify a specific field by typing its name. <code>field_name:search parameter</code></p>"},{"location":"delete/MIP_Tables/advanced_search/#boosts","title":"Boosts","text":"<p>When searching multiple parameters, e.g. <code>cmip cmor</code> we are likely to get more matches on cmip, which means that the results may not be sorted in the way we want them. To counter act this we can boost the importance of a keyword in the search rankings: <code>cmip cmor^10</code></p>"},{"location":"delete/MIP_Tables/advanced_search/#fuzzy-matches","title":"Fuzzy matches","text":"<p>We can apply the levenshtein distance matching algorithm to give us words that resemble what we type, but may not be complete matches: <code>cmar~1</code> Here the number after the tide tells us we want to match all occurances with 1 character different from our query. </p>"},{"location":"delete/MIP_Tables/advanced_search/#term-presence","title":"Term presence","text":"<p>Much like an online search engine we can provide multiple parameters, but mandate some words have to appear in all matches, and ensure others do not appear in any:  <code>+all some -none</code> If we want multiple words to appear in all returned results we can add these in sequence <code>+mip +cmor</code></p>"},{"location":"delete/mailing_lists/whitelisting/","title":"Whitelisting the CMIP server","text":"<p>As our mail server is relatively new, although all the required message headers and metadata have been set, emails may occasionally end up in the spam/quarantine folder. </p> <p>If you are having trouble recieving emails, or these are being flagged, you may have to add the WCRP-cmip domain to your whitelist. The most common solutions are provided below, although should you have any others, feel free to contact us, and we will add them to the list. </p>"},{"location":"delete/mailing_lists/whitelisting/#microsoft","title":"Microsoft","text":""},{"location":"delete/mailing_lists/whitelisting/#1-open-the-safe-senders-list","title":"1. Open the safe-senders list","text":"<p>https://outlook.office365.com/mail/options/mail/junkEmail/safeSendersDomainsV2</p>"},{"location":"delete/mailing_lists/whitelisting/#2-add-the-wcrp-cmip-domain","title":"2. Add the WCRP-CMIP domain","text":"<p>We now need to add our domain to be whitelisted. The domains are: <pre><code>wcrp-cmip.org\nmail.wcrp-cmip.org\n</code></pre></p> <p></p>"}]}